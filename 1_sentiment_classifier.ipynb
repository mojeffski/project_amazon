{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import re\n",
    "import os\n",
    "#from dotenv import load_dotenv\n",
    "from huggingface_hub import login\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import pipeline\n",
    "import sqlite3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: python-dotenv in /usr/local/lib/python3.11/dist-packages (1.0.1)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c60dfd4f18ff4551a500e69da64525aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "!pip install python-dotenv\n",
    "from dotenv import load_dotenv\n",
    "from huggingface_hub import login  # Or use your method of authentication\n",
    "\n",
    "# Load environment variables from the .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Access the Hugging Face token\n",
    "hf_token = os.getenv(\"HF_TOKEN\")\n",
    "\n",
    "# Use the token to authenticate with Hugging Face\n",
    "login(token=hf_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing individual functions from the cleaning.py module\n",
    "def label_sentiment(rating):\n",
    "    if rating >= 4:\n",
    "        return 2  # Positive\n",
    "    elif rating == 3:\n",
    "        return 1  # Neutral\n",
    "    else:\n",
    "        return 0  # Negative\n",
    "    \n",
    "def truncate_review(review, max_length=512):\n",
    "    return review[:max_length]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract overall accuracy and precision values for each group\n",
    "def extract_metrics(classification_report):\n",
    "    # Extract overall accuracy\n",
    "    accuracy = re.search(r'accuracy\\s+([\\d.]+)', classification_report)\n",
    "    overall_accuracy = float(accuracy.group(1)) if accuracy else None\n",
    "    \n",
    "    # Extract precision values for each group\n",
    "    precision_values = {}\n",
    "    lines = classification_report.splitlines()\n",
    "    \n",
    "    # Iterate over each line in the classification report\n",
    "    for line in lines:\n",
    "        # Identify lines that contain precision values for the groups (negative, positive, neutral, or stars)\n",
    "        if 'negative' in line or 'positive' in line or 'neutral' in line or 'star' in line:\n",
    "            parts = line.split()\n",
    "            \n",
    "            # Handle multi-word group names like '1 star' or '2 stars'\n",
    "            if parts[1] == 'star' or parts[1] == 'stars':\n",
    "                group = parts[0] + ' ' + parts[1]  # Example: '1 star', '2 stars'\n",
    "                precision = float(parts[2])  # Precision value is at index 2\n",
    "            else:\n",
    "                group = parts[0]  # Example: 'negative', 'positive', 'neutral'\n",
    "                precision = float(parts[1])  # Precision value is at index 1\n",
    "\n",
    "            # Store the precision value for the group\n",
    "            precision_values[group] = precision\n",
    "    \n",
    "    return overall_accuracy, precision_values\n",
    "\n",
    "# Function to store accuracy and precision metrics in the database\n",
    "def store_metrics_in_db(model_name, dataset_type, overall_accuracy, precision_values):\n",
    "    with sqlite3.connect('flask_project/data/blogposts.db', timeout=30) as conn:\n",
    "        cursor = conn.cursor()\n",
    "\n",
    "        # Create a table if it doesn't exist for storing the metrics\n",
    "        table_name = f\"{model_name}_{dataset_type}_metrics\"\n",
    "        cursor.execute(f\"\"\"\n",
    "            CREATE TABLE IF NOT EXISTS {table_name} (\n",
    "                group_name TEXT,\n",
    "                precision REAL,\n",
    "                overall_accuracy REAL\n",
    "            )\n",
    "        \"\"\")\n",
    "\n",
    "        # Insert overall accuracy\n",
    "        cursor.execute(f\"\"\"\n",
    "            INSERT INTO {table_name} (group_name, overall_accuracy)\n",
    "            VALUES (?, ?)\n",
    "        \"\"\", ('overall', overall_accuracy))\n",
    "\n",
    "        # Insert precision values for each group\n",
    "        for group, precision in precision_values.items():\n",
    "            cursor.execute(f\"\"\"\n",
    "                INSERT INTO {table_name} (group_name, precision)\n",
    "                VALUES (?, ?)\n",
    "            \"\"\", (group, precision))\n",
    "\n",
    "        # Commit the changes\n",
    "        conn.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_109/2435312577.py:3: DtypeWarning: Columns (1,10) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(project_path)\n"
     ]
    }
   ],
   "source": [
    "#path for dataset \n",
    "project_path = 'dataset/original/1429_1.csv'\n",
    "df = pd.read_csv(project_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_109/4066475600.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_clean.rename(columns={'reviews.text':'reviews_text'},inplace=True)# Display the first few rows of the cleaned data\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reviews_text</th>\n",
       "      <th>labels</th>\n",
       "      <th>labels_old</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>This product so far has not disappointed. My c...</td>\n",
       "      <td>2</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>great for beginner or experienced person. Boug...</td>\n",
       "      <td>2</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Inexpensive tablet for him to use and learn on...</td>\n",
       "      <td>2</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I've had my Fire HD 8 two weeks now and I love...</td>\n",
       "      <td>2</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I bought this for my grand daughter when she c...</td>\n",
       "      <td>2</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34655</th>\n",
       "      <td>This is not appreciably faster than any other ...</td>\n",
       "      <td>1</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34656</th>\n",
       "      <td>Amazon should include this charger with the Ki...</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34657</th>\n",
       "      <td>Love my Kindle Fire but I am really disappoint...</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34658</th>\n",
       "      <td>I was surprised to find it did not come with a...</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34659</th>\n",
       "      <td>to spite the fact that i have nothing but good...</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>34626 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            reviews_text  labels  labels_old\n",
       "0      This product so far has not disappointed. My c...       2         5.0\n",
       "1      great for beginner or experienced person. Boug...       2         5.0\n",
       "2      Inexpensive tablet for him to use and learn on...       2         5.0\n",
       "3      I've had my Fire HD 8 two weeks now and I love...       2         4.0\n",
       "4      I bought this for my grand daughter when she c...       2         5.0\n",
       "...                                                  ...     ...         ...\n",
       "34655  This is not appreciably faster than any other ...       1         3.0\n",
       "34656  Amazon should include this charger with the Ki...       0         1.0\n",
       "34657  Love my Kindle Fire but I am really disappoint...       0         1.0\n",
       "34658  I was surprised to find it did not come with a...       0         1.0\n",
       "34659  to spite the fact that i have nothing but good...       0         1.0\n",
       "\n",
       "[34626 rows x 3 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Do minimal data cleaning\n",
    "df['labels'] = df['reviews.rating'].apply(label_sentiment)\n",
    "df['labels_old'] = df['reviews.rating']\n",
    "#exclude empty entries\n",
    "df = df[df['reviews.text'].notnull()]\n",
    "\n",
    "# Keep only the columns we need (text and sentiment)\n",
    "df_clean = df[['reviews.text', 'labels','labels_old']]\n",
    "df_clean.rename(columns={'reviews.text':'reviews_text'},inplace=True)# Display the first few rows of the cleaned data\n",
    "df_clean = df_clean.dropna()\n",
    "df_clean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.utils import resample\n",
    "\n",
    "# Separate the classes\n",
    "df_majority = df_clean[df_clean['labels'] == 2]\n",
    "df_minority_1 = df_clean[df_clean['labels'] == 1]\n",
    "df_minority_0 = df_clean[df_clean['labels'] == 0]\n",
    "\n",
    "# Resample minority classes (oversample class 0 and 1)\n",
    "df_minority_1_upsampled = resample(df_minority_1, \n",
    "                                   replace=True,    # sample with replacement\n",
    "                                   n_samples=1499,  # match desired size\n",
    "                                   random_state=42)\n",
    "\n",
    "df_minority_0_upsampled = resample(df_minority_0, \n",
    "                                   replace=True,    # sample with replacement\n",
    "                                   n_samples=1499,  # match desired size\n",
    "                                   random_state=42)\n",
    "\n",
    "# Combine the resampled data\n",
    "df_balanced = pd.concat([df_majority, df_minority_1_upsampled, df_minority_0_upsampled])\n",
    "\n",
    "# Shuffle the dataset\n",
    "df_balanced = df_balanced.sample(frac=1, random_state=42).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming df_clean and df_balanced are your original and balanced dataframes\n",
    "#sample_size = 0.02  # Adjust this as needed\n",
    "\n",
    "# Sample 10% from each class in the 'labels_old' column\n",
    "#df_clean = df_clean.groupby('labels_old').apply(lambda x: x.sample(frac=sample_size, random_state=42)).reset_index(drop=True)\n",
    "#df_balanced = df_balanced.groupby('labels_old').apply(lambda x: x.sample(frac=sample_size, random_state=42)).reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the original distribution: labels_old\n",
      "5.0    23774\n",
      "4.0     8541\n",
      "3.0     1499\n",
      "1.0      410\n",
      "2.0      402\n",
      "Name: count, dtype: int64\n",
      "the resampled distribution: labels_old\n",
      "5.0    23774\n",
      "4.0     8541\n",
      "3.0     1499\n",
      "2.0      776\n",
      "1.0      723\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Check the new distribution\n",
    "print('the original distribution:',df_clean['labels_old'].value_counts())\n",
    "print('the resampled distribution:',df_balanced['labels_old'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Train (60%), Test (20%), Validation (20%)\n",
    "train_data_orig, temp_orig = train_test_split(df_clean, test_size=0.4, random_state=42)\n",
    "test_data_orig, val_data_orig = train_test_split(temp_orig, test_size=0.5, random_state=42)\n",
    "\n",
    "# Train (60%), Test (20%), Validation (20%)\n",
    "train_data_balanced, temp_balanced = train_test_split(df_balanced, test_size=0.4, random_state=42)\n",
    "test_data_balanced, val_data_balanced = train_test_split(temp_balanced, test_size=0.5, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_reviews_orig = [truncate_review(review) for review in test_data_orig['reviews_text'].tolist()]\n",
    "full_reviews_balanced = [truncate_review(review) for review in test_data_balanced['reviews_text'].tolist()]\n",
    "\n",
    "list_full_reviews = {'original': [full_reviews_orig,test_data_orig],\n",
    "                     'balanced':[full_reviews_balanced,test_data_balanced]\n",
    "                     }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. sentiment classifier "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - Create a model for classification of customers' reviews (the textual content of the reviews) into positive, neutral, or negative.\n",
    " 1. create a testing and training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 0 if torch.cuda.is_available() else -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Mapping for true labels (in case they are numerical in test_data)\n",
    "true_label_mapping = {\n",
    "    'roberta_scale':{\n",
    "    0: 'negative',\n",
    "    1: 'neutral',\n",
    "    2: 'positive'\n",
    "            },\n",
    "    'amazon_scale':{\n",
    "    1: '1 star',\n",
    "    2: '2 stars',\n",
    "    3: '3 stars',\n",
    "    4: '4 stars',\n",
    "    5: '5 stars'\n",
    "        }\n",
    "    }\n",
    "# Load pre-trained sentiment analysis pipeline\n",
    "model_eval_results = {'roberta':{},\n",
    "                      'amazon':{}\n",
    "                      }\n",
    "\n",
    "sentiment_models = {'roberta': {'hf_id':'cardiffnlp/twitter-roberta-base-sentiment-latest','num_labels':3},\n",
    "                    'amazon' : {'hf_id':'LiYuan/amazon-review-sentiment-analysis','num_labels':5}\n",
    "                    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Roberta\n",
    "list_full_reviews = {'original': [full_reviews_orig,test_data_orig],\n",
    "                     'balanced':[full_reviews_balanced,test_data_balanced]\n",
    "                     }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b7b03ab92d343958d3d0195b186f979",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/929 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "33b46dd003ff4de5869e2ea72328eeac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/501M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at cardiffnlp/twitter-roberta-base-sentiment-latest were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be0296a082494f5aad39a1f631105091",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f8ae54e77814ca5a3c82a6be21a787e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c886cb8f99949fab803aac5ccf1dac4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/239 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "019a1a04502e4b9188e970fa5facd9f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.23k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef1168780bca4cef9b44fc710b61ba5b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/670M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d69651063c1c4f5a80c5dd3f03e68cd1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/556 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "594fb5fd725141a5bddf4aa56ab933d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/872k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9cd2d19bae24495da8209a1646f20a86",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/2.56M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f13410bfdc4a4dbc977b00b6dd274023",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sentiment_roberta = pipeline('sentiment-analysis', model=sentiment_models['roberta']['hf_id'], device=device)\n",
    "sentiment_amazon = pipeline('sentiment-analysis', model=sentiment_models['amazon']['hf_id'], device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "classification_results = {'roberta':{'original':{},'balanced':{}},\n",
    "                          'amazon':{'original':{},'balanced':{}},\n",
    "                          \n",
    "                          }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Origianl Dataaset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.1: Amazon model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      1 star       0.40      0.80      0.53        96\n",
      "     2 stars       0.24      0.21      0.22        78\n",
      "     3 stars       0.37      0.29      0.32       286\n",
      "     4 stars       0.63      0.22      0.32      1701\n",
      "     5 stars       0.78      0.95      0.86      4764\n",
      "\n",
      "    accuracy                           0.74      6925\n",
      "   macro avg       0.48      0.49      0.45      6925\n",
      "weighted avg       0.71      0.74      0.69      6925\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "results = sentiment_amazon(list_full_reviews['original'][0])\n",
    "predicted_sentiments = [result['label'] for result in results]\n",
    "\n",
    "# Map true labels\n",
    "true_sentiments = list_full_reviews['original'][1]['labels_old'].map(true_label_mapping['amazon_scale']).tolist()\n",
    "\n",
    "classification_matrix = classification_report(true_sentiments, predicted_sentiments, target_names=list(true_label_mapping['amazon_scale'].values()))\n",
    "conf_matrix = confusion_matrix(true_sentiments, predicted_sentiments, labels=list(true_label_mapping['amazon_scale'].values()))\n",
    "\n",
    "#Add results:\n",
    "classification_results['amazon']['original'].update({'classification':classification_matrix,\n",
    "                                                     'confusion_matrix':conf_matrix})\n",
    "print('done')\n",
    "print(classification_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.2: Roberta model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.36      0.73      0.48       174\n",
      "     neutral       0.14      0.20      0.16       286\n",
      "    positive       0.97      0.93      0.95      6465\n",
      "\n",
      "    accuracy                           0.89      6925\n",
      "   macro avg       0.49      0.62      0.53      6925\n",
      "weighted avg       0.92      0.89      0.90      6925\n",
      "\n"
     ]
    }
   ],
   "source": [
    "results = sentiment_roberta(list_full_reviews['original'][0])\n",
    "predicted_sentiments = [result['label'] for result in results]\n",
    "\n",
    "# Map true labels\n",
    "true_sentiments = list_full_reviews['original'][1]['labels'].map(true_label_mapping['roberta_scale']).tolist()\n",
    "\n",
    "classification_matrix = classification_report(true_sentiments, predicted_sentiments, target_names=list(true_label_mapping['roberta_scale'].values()))\n",
    "conf_matrix = confusion_matrix(true_sentiments, predicted_sentiments, labels=list(true_label_mapping['roberta_scale'].values()))\n",
    "\n",
    "#Add results:\n",
    "classification_results['roberta']['original'].update({'classification':classification_matrix,\n",
    "                                                     'confusion_matrix':conf_matrix})\n",
    "print('done')\n",
    "print(classification_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2: Balanced Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.1: Amazon model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      1 star       0.42      0.72      0.53       165\n",
      "     2 stars       0.25      0.15      0.19       151\n",
      "     3 stars       0.32      0.25      0.28       299\n",
      "     4 stars       0.62      0.20      0.31      1751\n",
      "     5 stars       0.76      0.95      0.85      4697\n",
      "\n",
      "    accuracy                           0.72      7063\n",
      "   macro avg       0.47      0.46      0.43      7063\n",
      "weighted avg       0.69      0.72      0.67      7063\n",
      "\n"
     ]
    }
   ],
   "source": [
    "results = sentiment_amazon(list_full_reviews['balanced'][0])\n",
    "predicted_sentiments = [result['label'] for result in results]\n",
    "\n",
    "# Map true labels\n",
    "true_sentiments = list_full_reviews['balanced'][1]['labels_old'].map(true_label_mapping['amazon_scale']).tolist()\n",
    "\n",
    "classification_matrix = classification_report(true_sentiments, predicted_sentiments, target_names=list(true_label_mapping['amazon_scale'].values()))\n",
    "conf_matrix = confusion_matrix(true_sentiments, predicted_sentiments, labels=list(true_label_mapping['amazon_scale'].values()))\n",
    "\n",
    "#Add results:\n",
    "classification_results['amazon']['balanced'].update({'classification':classification_matrix,\n",
    "                                                     'confusion_matrix':conf_matrix})\n",
    "print('done')\n",
    "print(classification_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7063"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(predicted_sentiments)\n",
    "len(true_sentiments)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.2: Roberta model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.49      0.75      0.60       316\n",
      "     neutral       0.16      0.24      0.19       299\n",
      "    positive       0.97      0.93      0.95      6448\n",
      "\n",
      "    accuracy                           0.89      7063\n",
      "   macro avg       0.54      0.64      0.58      7063\n",
      "weighted avg       0.92      0.89      0.90      7063\n",
      "\n"
     ]
    }
   ],
   "source": [
    "results = sentiment_roberta(list_full_reviews['balanced'][0])\n",
    "predicted_sentiments = [result['label'] for result in results]\n",
    "\n",
    "# Map true labels\n",
    "true_sentiments = list_full_reviews['balanced'][1]['labels'].map(true_label_mapping['roberta_scale']).tolist()\n",
    "\n",
    "classification_matrix = classification_report(true_sentiments, predicted_sentiments, target_names=list(true_label_mapping['roberta_scale'].values()))\n",
    "conf_matrix = confusion_matrix(true_sentiments, predicted_sentiments, labels=list(true_label_mapping['roberta_scale'].values()))\n",
    "\n",
    "#Add results:\n",
    "classification_results['roberta']['balanced'].update({'classification':classification_matrix,\n",
    "                                                     'confusion_matrix':conf_matrix})\n",
    "print('done')\n",
    "print(classification_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored metrics for Model: roberta, Dataset: original\n",
      "Stored metrics for Model: roberta, Dataset: balanced\n",
      "Stored metrics for Model: amazon, Dataset: original\n",
      "Stored metrics for Model: amazon, Dataset: balanced\n"
     ]
    }
   ],
   "source": [
    "# Extract and store metrics in the database for both models\n",
    "for model, data in classification_results.items():\n",
    "    for dataset_type, result in data.items():\n",
    "        accuracy, precision = extract_metrics(result['classification'])\n",
    "        store_metrics_in_db(model, dataset_type, accuracy, precision)\n",
    "        print(f\"Stored metrics for Model: {model}, Dataset: {dataset_type}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Fine tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## new attempt:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f2e24fe170914e2b8bc69a07dda4f4cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/24719 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5192b5baeebb407e85f598113d3f4f9f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/5297 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b5d51f5fe56a440ba1a29f73b02c4010",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/5297 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c276533e496d41d1a5d8ee16cd74a0e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/5297 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "11690fcbee124b24b6860125aebaf03e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/5297 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
    "import torch\n",
    "\n",
    "# Initialize the tokenizer (example: for roberta-base model)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"roberta-base\")\n",
    "\n",
    "# Tokenizer function for text tokenization\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples['reviews_text'], padding=\"max_length\", truncation=True, max_length=128)\n",
    "\n",
    "# Apply tokenization to datasets\n",
    "train_data, val_test_data = train_test_split(df_balanced, test_size=0.3, random_state=42)\n",
    "val_data, test_data = train_test_split(val_test_data, test_size=0.5, random_state=42)\n",
    "\n",
    "train_dataset = Dataset.from_pandas(train_data)\n",
    "val_dataset = Dataset.from_pandas(val_data)\n",
    "test_dataset = Dataset.from_pandas(test_data)\n",
    "\n",
    "# Tokenize the datasets\n",
    "train_dataset = train_dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "# Create separate validation datasets for roberta and amazon\n",
    "val_dataset_roberta = val_dataset.map(tokenize_function, batched=True)\n",
    "val_dataset_amazon = val_dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "test_dataset = test_dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "# Set format for roberta validation dataset\n",
    "val_dataset_roberta.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])\n",
    "\n",
    "# Set format for amazon validation dataset (use labels_old instead of labels)\n",
    "val_dataset_amazon = val_dataset_amazon.map(lambda examples: {'labels': examples['labels_old']})\n",
    "val_dataset_amazon.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])\n",
    "\n",
    "# Set format for train and test datasets\n",
    "train_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])\n",
    "test_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(set(val_dataset_amazon['labels_old'])) <= 5, \"Amazon model expects 5 classes!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
    "\n",
    "sentiment_models = {'roberta': {'hf_id':'cardiffnlp/twitter-roberta-base-sentiment-latest','num_labels':3},\n",
    "                    'amazon' : {'hf_id':'LiYuan/amazon-review-sentiment-analysis','num_labels':5}\n",
    "                    }\n",
    "\n",
    "# Load pre-trained tokenizer and model\n",
    "model_amazon = sentiment_models['roberta']['hf_id']\n",
    "model_roberta = sentiment_models['amazon']['hf_id']\n",
    "#tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "#model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to compute evaluation metrics\n",
    "def compute_metrics(p):\n",
    "    predictions, labels = p\n",
    "    preds = predictions.argmax(-1)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='weighted')\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    return {\n",
    "        'accuracy': acc,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1': f1\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
    "import os\n",
    "import torch\n",
    "\n",
    "# Step 1: Fine-tune and save models\n",
    "def fine_tune_and_save_model(model_name, train_dataset, val_dataset, num_labels):\n",
    "    # Load pre-trained model and tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=num_labels)\n",
    "\n",
    "    # Move the model to the correct device\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "\n",
    "    # Tokenize the datasets\n",
    "    train_dataset = train_dataset.map(tokenize_function, batched=True)\n",
    "    val_dataset = val_dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "    # Training Arguments\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=f\"./results/{model_name}\",\n",
    "        evaluation_strategy=\"steps\",\n",
    "        save_strategy=\"steps\",\n",
    "        save_total_limit=2,\n",
    "        learning_rate=2e-5,\n",
    "        per_device_train_batch_size=64,\n",
    "        per_device_eval_batch_size=64,\n",
    "        num_train_epochs=3,\n",
    "        weight_decay=0.01,\n",
    "        logging_dir=f\"./logs/{model_name}\",\n",
    "        load_best_model_at_end=True,\n",
    "        no_cuda=(not torch.cuda.is_available())  # Use CPU if no CUDA is available\n",
    "    )\n",
    "\n",
    "    # Initialize the Trainer\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=val_dataset,\n",
    "        compute_metrics=compute_metrics\n",
    "    )\n",
    "\n",
    "    # Fine-tune the model\n",
    "    trainer.train()\n",
    "\n",
    "    # Save the fine-tuned model\n",
    "    trainer.save_model(f\"./fine_tuned_model/{model_name}\")\n",
    "    tokenizer.save_pretrained(f\"./fine_tuned_model/{model_name}\")\n",
    "\n",
    "    print(f\"Fine-tuned model {model_name} saved successfully.\")\n",
    "    return trainer  # Return the trainer for further evaluation\n",
    "\n",
    "    # Return trainer for later evaluation\n",
    "    return trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Fine-tune both models (Roberta and Amazon)\n",
    "for model_key, model_info in sentiment_models.items():\n",
    "    model_name = model_info['hf_id']\n",
    "    num_labels = model_info['num_labels']\n",
    "    \n",
    "    # Fine-tune the model and save it\n",
    "    trainer = fine_tune_and_save_model(model_name, train_dataset, val_dataset, num_labels)\n",
    "    \n",
    "    # Store the trained models before moving to the next step\n",
    "    print(f\"Model {model_name} has been fine-tuned and saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# Step 2: Evaluate the model and store metrics\n",
    "def evaluate_and_store_metrics(trainer, model_name, dataset, val_dataset, true_labels_mapping):\n",
    "    # Evaluate the model on the validation dataset\n",
    "    predictions = trainer.predict(val_dataset)\n",
    "    preds = predictions.predictions.argmax(-1)  # Get the predicted labels\n",
    "\n",
    "    # Use 'labels' for Roberta and 'labels_old' for Amazon as defined in the datasets\n",
    "    true_labels = val_dataset['labels'].numpy()\n",
    "\n",
    "    # Generate the classification report for individual class metrics\n",
    "    report = classification_report(true_labels, preds, target_names=list(true_labels_mapping.values()))\n",
    "    print(f\"Classification Report for {model_name}:\\n{report}\")\n",
    "    \n",
    "    # Calculate and print the confusion matrix\n",
    "    conf_matrix = confusion_matrix(true_labels, preds)\n",
    "    print(f\"Confusion Matrix for {model_name}:\\n{conf_matrix}\")\n",
    "\n",
    "    # Store the classification report and confusion matrix in the classification_results dictionary\n",
    "    classification_results[model_name][dataset].update({\n",
    "        'classification': report,\n",
    "        'confusion_matrix': conf_matrix\n",
    "    })\n",
    "\n",
    "    # Extract overall accuracy and precision values from the classification report\n",
    "    overall_accuracy, precision_values = extract_metrics(report)\n",
    "    \n",
    "    # Store metrics (overall accuracy and precision) in the SQLite database\n",
    "    store_metrics_in_db(model_name, dataset, overall_accuracy, precision_values)\n",
    "\n",
    "    # You can optionally store or log the confusion matrix somewhere else (or print it out)\n",
    "    # You might want to extend this part to store the confusion matrix in the database if needed.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.environ[\"WANDB_MODE\"] = \"disabled\"\n",
    "torch.backends.mps.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 0 if torch.cuda.is_available() else -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
    "\n",
    "\n",
    "# Define paths to your fine-tuned models\n",
    "roberta_model_path = \"./fine_tuned_model/roberta\"\n",
    "amazon_model_path = \"./fine_tuned_model/amazon\"\n",
    "\n",
    "# Load pre-trained tokenizer and model for Roberta\n",
    "tokenizer_roberta = AutoTokenizer.from_pretrained(roberta_model_path)\n",
    "model_roberta = AutoModelForSequenceClassification.from_pretrained(roberta_model_path)\n",
    "\n",
    "# Load pre-trained tokenizer and model for Amazon\n",
    "tokenizer_amazon = AutoTokenizer.from_pretrained(amazon_model_path)\n",
    "model_amazon = AutoModelForSequenceClassification.from_pretrained(amazon_model_path)\n",
    "\n",
    "# Now you can evaluate the models using the same Trainer and evaluation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define training arguments (you can reuse the ones used during fine-tuning)\n",
    "training_args = TrainingArguments(\n",
    "    per_device_eval_batch_size=64,\n",
    "    output_dir=\"./results/\",\n",
    "    logging_dir=\"./logs/\",\n",
    "    no_cuda=(not torch.cuda.is_available())  # Use CPU if no CUDA is available\n",
    ")\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
    "# Create the Trainer objects for evaluation\n",
    "trainer_roberta = Trainer(\n",
    "    model=model_roberta,\n",
    "    args=training_args,\n",
    "    eval_dataset=val_dataset_roberta\n",
    ")\n",
    "\n",
    "trainer_amazon = Trainer(\n",
    "    model=model_amazon,\n",
    "    args=training_args,\n",
    "    eval_dataset=val_dataset_amazon\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report for roberta:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.89      0.78      0.83       240\n",
      "     neutral       0.48      0.49      0.48       211\n",
      "    positive       0.98      0.98      0.98      4846\n",
      "\n",
      "    accuracy                           0.95      5297\n",
      "   macro avg       0.78      0.75      0.76      5297\n",
      "weighted avg       0.95      0.95      0.95      5297\n",
      "\n",
      "Confusion Matrix for roberta:\n",
      "[[ 186   34   20]\n",
      " [  18  103   90]\n",
      " [   6   77 4763]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Evaluate the models and store metrics\n",
    "evaluate_and_store_metrics(trainer_roberta, 'roberta', 'balanced', val_dataset_roberta, true_label_mapping['roberta_scale'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#evaluate_and_store_metrics(trainer_amazon, 'amazon', 'balanced', val_dataset_amazon, true_label_mapping['amazon_scale'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reviews_text</th>\n",
       "      <th>labels</th>\n",
       "      <th>labels_old</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>This product so far has not disappointed. My c...</td>\n",
       "      <td>2</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>great for beginner or experienced person. Boug...</td>\n",
       "      <td>2</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Inexpensive tablet for him to use and learn on...</td>\n",
       "      <td>2</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I've had my Fire HD 8 two weeks now and I love...</td>\n",
       "      <td>2</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I bought this for my grand daughter when she c...</td>\n",
       "      <td>2</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34655</th>\n",
       "      <td>This is not appreciably faster than any other ...</td>\n",
       "      <td>1</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34656</th>\n",
       "      <td>Amazon should include this charger with the Ki...</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34657</th>\n",
       "      <td>Love my Kindle Fire but I am really disappoint...</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34658</th>\n",
       "      <td>I was surprised to find it did not come with a...</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34659</th>\n",
       "      <td>to spite the fact that i have nothing but good...</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>34626 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            reviews_text  labels  labels_old\n",
       "0      This product so far has not disappointed. My c...       2         5.0\n",
       "1      great for beginner or experienced person. Boug...       2         5.0\n",
       "2      Inexpensive tablet for him to use and learn on...       2         5.0\n",
       "3      I've had my Fire HD 8 two weeks now and I love...       2         4.0\n",
       "4      I bought this for my grand daughter when she c...       2         5.0\n",
       "...                                                  ...     ...         ...\n",
       "34655  This is not appreciably faster than any other ...       1         3.0\n",
       "34656  Amazon should include this charger with the Ki...       0         1.0\n",
       "34657  Love my Kindle Fire but I am really disappoint...       0         1.0\n",
       "34658  I was surprised to find it did not come with a...       0         1.0\n",
       "34659  to spite the fact that i have nothing but good...       0         1.0\n",
       "\n",
       "[34626 rows x 3 columns]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### setting sentiment analysis on "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.dropna(subset=['reviews.text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "Cannot save file into a non-existent directory: 'datasets/interim'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[48], line 26\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m# Export the DataFrame with the new sentiment column to a CSV file in the 'datasets/interim' folder\u001b[39;00m\n\u001b[1;32m     25\u001b[0m export_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdatasets/interim/cleaned_with_sentiment_numeric.csv\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m---> 26\u001b[0m \u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexport_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset with sentiment column (0, 1, 2) saved to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mexport_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/util/_decorators.py:333\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    327\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m>\u001b[39m num_allow_args:\n\u001b[1;32m    328\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    329\u001b[0m         msg\u001b[38;5;241m.\u001b[39mformat(arguments\u001b[38;5;241m=\u001b[39m_format_argument_list(allow_args)),\n\u001b[1;32m    330\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[1;32m    331\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mfind_stack_level(),\n\u001b[1;32m    332\u001b[0m     )\n\u001b[0;32m--> 333\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/generic.py:3961\u001b[0m, in \u001b[0;36mNDFrame.to_csv\u001b[0;34m(self, path_or_buf, sep, na_rep, float_format, columns, header, index, index_label, mode, encoding, compression, quoting, quotechar, lineterminator, chunksize, date_format, doublequote, escapechar, decimal, errors, storage_options)\u001b[0m\n\u001b[1;32m   3950\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m, ABCDataFrame) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mto_frame()\n\u001b[1;32m   3952\u001b[0m formatter \u001b[38;5;241m=\u001b[39m DataFrameFormatter(\n\u001b[1;32m   3953\u001b[0m     frame\u001b[38;5;241m=\u001b[39mdf,\n\u001b[1;32m   3954\u001b[0m     header\u001b[38;5;241m=\u001b[39mheader,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3958\u001b[0m     decimal\u001b[38;5;241m=\u001b[39mdecimal,\n\u001b[1;32m   3959\u001b[0m )\n\u001b[0;32m-> 3961\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mDataFrameRenderer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mformatter\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_csv\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3962\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpath_or_buf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3963\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlineterminator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlineterminator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3964\u001b[0m \u001b[43m    \u001b[49m\u001b[43msep\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msep\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3965\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3966\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3967\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcompression\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3968\u001b[0m \u001b[43m    \u001b[49m\u001b[43mquoting\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquoting\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3969\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3970\u001b[0m \u001b[43m    \u001b[49m\u001b[43mindex_label\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindex_label\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3971\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3972\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchunksize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunksize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3973\u001b[0m \u001b[43m    \u001b[49m\u001b[43mquotechar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquotechar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3974\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdate_format\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdate_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3975\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdoublequote\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdoublequote\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3976\u001b[0m \u001b[43m    \u001b[49m\u001b[43mescapechar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mescapechar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3977\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3978\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/formats/format.py:1014\u001b[0m, in \u001b[0;36mDataFrameRenderer.to_csv\u001b[0;34m(self, path_or_buf, encoding, sep, columns, index_label, mode, compression, quoting, quotechar, lineterminator, chunksize, date_format, doublequote, escapechar, errors, storage_options)\u001b[0m\n\u001b[1;32m    993\u001b[0m     created_buffer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    995\u001b[0m csv_formatter \u001b[38;5;241m=\u001b[39m CSVFormatter(\n\u001b[1;32m    996\u001b[0m     path_or_buf\u001b[38;5;241m=\u001b[39mpath_or_buf,\n\u001b[1;32m    997\u001b[0m     lineterminator\u001b[38;5;241m=\u001b[39mlineterminator,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1012\u001b[0m     formatter\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfmt,\n\u001b[1;32m   1013\u001b[0m )\n\u001b[0;32m-> 1014\u001b[0m \u001b[43mcsv_formatter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1016\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m created_buffer:\n\u001b[1;32m   1017\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(path_or_buf, StringIO)\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/formats/csvs.py:251\u001b[0m, in \u001b[0;36mCSVFormatter.save\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    247\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    248\u001b[0m \u001b[38;5;124;03mCreate the writer & save.\u001b[39;00m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    250\u001b[0m \u001b[38;5;66;03m# apply compression and byte/text conversion\u001b[39;00m\n\u001b[0;32m--> 251\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    252\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    253\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    256\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompression\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    257\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    258\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m handles:\n\u001b[1;32m    259\u001b[0m     \u001b[38;5;66;03m# Note: self.encoding is irrelevant here\u001b[39;00m\n\u001b[1;32m    260\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwriter \u001b[38;5;241m=\u001b[39m csvlib\u001b[38;5;241m.\u001b[39mwriter(\n\u001b[1;32m    261\u001b[0m         handles\u001b[38;5;241m.\u001b[39mhandle,\n\u001b[1;32m    262\u001b[0m         lineterminator\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlineterminator,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    267\u001b[0m         quotechar\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mquotechar,\n\u001b[1;32m    268\u001b[0m     )\n\u001b[1;32m    270\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_save()\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/common.py:749\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    747\u001b[0m \u001b[38;5;66;03m# Only for write methods\u001b[39;00m\n\u001b[1;32m    748\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode \u001b[38;5;129;01mand\u001b[39;00m is_path:\n\u001b[0;32m--> 749\u001b[0m     \u001b[43mcheck_parent_directory\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    751\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m compression:\n\u001b[1;32m    752\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m compression \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mzstd\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    753\u001b[0m         \u001b[38;5;66;03m# compression libraries do not like an explicit text-mode\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/common.py:616\u001b[0m, in \u001b[0;36mcheck_parent_directory\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m    614\u001b[0m parent \u001b[38;5;241m=\u001b[39m Path(path)\u001b[38;5;241m.\u001b[39mparent\n\u001b[1;32m    615\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m parent\u001b[38;5;241m.\u001b[39mis_dir():\n\u001b[0;32m--> 616\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(\u001b[38;5;124mrf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot save file into a non-existent directory: \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mparent\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mOSError\u001b[0m: Cannot save file into a non-existent directory: 'datasets/interim'"
     ]
    }
   ],
   "source": [
    "# Make sure to run the model on CPU if GPU is not available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model_roberta.to(device)\n",
    "\n",
    "# Function to get predictions from the model in batches\n",
    "def predict_sentiment_in_batches(texts, model, tokenizer, batch_size=32):\n",
    "    all_predictions = []\n",
    "    for i in range(0, len(texts), batch_size):\n",
    "        batch_texts = texts[i:i+batch_size]\n",
    "        inputs = tokenizer(batch_texts, padding=True, truncation=True, return_tensors=\"pt\", max_length=128).to(device)\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "        predictions = torch.argmax(outputs.logits, dim=-1)\n",
    "        all_predictions.extend(predictions.cpu().numpy())\n",
    "    return all_predictions\n",
    "\n",
    "# Apply the model in batches to avoid memory issues\n",
    "df['sentiment'] = predict_sentiment_in_batches(df['reviews.text'].tolist(), model_roberta, tokenizer_roberta)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset with sentiment column (0, 1, 2) saved to dataset/interim/cleaned_with_sentiment_numeric.csv\n"
     ]
    }
   ],
   "source": [
    "# Export the DataFrame with the new sentiment column to a CSV file in the 'datasets/interim' folder\n",
    "export_path = 'dataset/interim/cleaned_with_sentiment_numeric.csv'\n",
    "df.to_csv(export_path, index=False)\n",
    "\n",
    "print(f\"Dataset with sentiment column (0, 1, 2) saved to {export_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>name</th>\n",
       "      <th>asins</th>\n",
       "      <th>brand</th>\n",
       "      <th>categories</th>\n",
       "      <th>keys</th>\n",
       "      <th>manufacturer</th>\n",
       "      <th>reviews.date</th>\n",
       "      <th>reviews.dateAdded</th>\n",
       "      <th>reviews.dateSeen</th>\n",
       "      <th>...</th>\n",
       "      <th>reviews.rating</th>\n",
       "      <th>reviews.sourceURLs</th>\n",
       "      <th>reviews.text</th>\n",
       "      <th>reviews.title</th>\n",
       "      <th>reviews.userCity</th>\n",
       "      <th>reviews.userProvince</th>\n",
       "      <th>reviews.username</th>\n",
       "      <th>labels</th>\n",
       "      <th>labels_old</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AVqkIhwDv8e3D1O-lebb</td>\n",
       "      <td>All-New Fire HD 8 Tablet, 8 HD Display, Wi-Fi,...</td>\n",
       "      <td>B01AHB9CN2</td>\n",
       "      <td>Amazon</td>\n",
       "      <td>Electronics,iPad &amp; Tablets,All Tablets,Fire Ta...</td>\n",
       "      <td>841667104676,amazon/53004484,amazon/b01ahb9cn2...</td>\n",
       "      <td>Amazon</td>\n",
       "      <td>2017-01-13T00:00:00.000Z</td>\n",
       "      <td>2017-07-03T23:33:15Z</td>\n",
       "      <td>2017-06-07T09:04:00.000Z,2017-04-30T00:45:00.000Z</td>\n",
       "      <td>...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>http://reviews.bestbuy.com/3545/5620406/review...</td>\n",
       "      <td>This product so far has not disappointed. My c...</td>\n",
       "      <td>Kindle</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Adapter</td>\n",
       "      <td>2</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AVqkIhwDv8e3D1O-lebb</td>\n",
       "      <td>All-New Fire HD 8 Tablet, 8 HD Display, Wi-Fi,...</td>\n",
       "      <td>B01AHB9CN2</td>\n",
       "      <td>Amazon</td>\n",
       "      <td>Electronics,iPad &amp; Tablets,All Tablets,Fire Ta...</td>\n",
       "      <td>841667104676,amazon/53004484,amazon/b01ahb9cn2...</td>\n",
       "      <td>Amazon</td>\n",
       "      <td>2017-01-13T00:00:00.000Z</td>\n",
       "      <td>2017-07-03T23:33:15Z</td>\n",
       "      <td>2017-06-07T09:04:00.000Z,2017-04-30T00:45:00.000Z</td>\n",
       "      <td>...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>http://reviews.bestbuy.com/3545/5620406/review...</td>\n",
       "      <td>great for beginner or experienced person. Boug...</td>\n",
       "      <td>very fast</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>truman</td>\n",
       "      <td>2</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AVqkIhwDv8e3D1O-lebb</td>\n",
       "      <td>All-New Fire HD 8 Tablet, 8 HD Display, Wi-Fi,...</td>\n",
       "      <td>B01AHB9CN2</td>\n",
       "      <td>Amazon</td>\n",
       "      <td>Electronics,iPad &amp; Tablets,All Tablets,Fire Ta...</td>\n",
       "      <td>841667104676,amazon/53004484,amazon/b01ahb9cn2...</td>\n",
       "      <td>Amazon</td>\n",
       "      <td>2017-01-13T00:00:00.000Z</td>\n",
       "      <td>2017-07-03T23:33:15Z</td>\n",
       "      <td>2017-06-07T09:04:00.000Z,2017-04-30T00:45:00.000Z</td>\n",
       "      <td>...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>http://reviews.bestbuy.com/3545/5620406/review...</td>\n",
       "      <td>Inexpensive tablet for him to use and learn on...</td>\n",
       "      <td>Beginner tablet for our 9 year old son.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>DaveZ</td>\n",
       "      <td>2</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AVqkIhwDv8e3D1O-lebb</td>\n",
       "      <td>All-New Fire HD 8 Tablet, 8 HD Display, Wi-Fi,...</td>\n",
       "      <td>B01AHB9CN2</td>\n",
       "      <td>Amazon</td>\n",
       "      <td>Electronics,iPad &amp; Tablets,All Tablets,Fire Ta...</td>\n",
       "      <td>841667104676,amazon/53004484,amazon/b01ahb9cn2...</td>\n",
       "      <td>Amazon</td>\n",
       "      <td>2017-01-13T00:00:00.000Z</td>\n",
       "      <td>2017-07-03T23:33:15Z</td>\n",
       "      <td>2017-06-07T09:04:00.000Z,2017-04-30T00:45:00.000Z</td>\n",
       "      <td>...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>http://reviews.bestbuy.com/3545/5620406/review...</td>\n",
       "      <td>I've had my Fire HD 8 two weeks now and I love...</td>\n",
       "      <td>Good!!!</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Shacks</td>\n",
       "      <td>2</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AVqkIhwDv8e3D1O-lebb</td>\n",
       "      <td>All-New Fire HD 8 Tablet, 8 HD Display, Wi-Fi,...</td>\n",
       "      <td>B01AHB9CN2</td>\n",
       "      <td>Amazon</td>\n",
       "      <td>Electronics,iPad &amp; Tablets,All Tablets,Fire Ta...</td>\n",
       "      <td>841667104676,amazon/53004484,amazon/b01ahb9cn2...</td>\n",
       "      <td>Amazon</td>\n",
       "      <td>2017-01-12T00:00:00.000Z</td>\n",
       "      <td>2017-07-03T23:33:15Z</td>\n",
       "      <td>2017-06-07T09:04:00.000Z,2017-04-30T00:45:00.000Z</td>\n",
       "      <td>...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>http://reviews.bestbuy.com/3545/5620406/review...</td>\n",
       "      <td>I bought this for my grand daughter when she c...</td>\n",
       "      <td>Fantastic Tablet for kids</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>explore42</td>\n",
       "      <td>2</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34655</th>\n",
       "      <td>AVpfiBlyLJeJML43-4Tp</td>\n",
       "      <td>NaN</td>\n",
       "      <td>B006GWO5WK</td>\n",
       "      <td>Amazon</td>\n",
       "      <td>Computers/Tablets &amp; Networking,Tablet &amp; eBook ...</td>\n",
       "      <td>newamazonkindlefirehd9wpowerfastadaptercharger...</td>\n",
       "      <td>Amazon Digital Services, Inc</td>\n",
       "      <td>2012-09-18T00:00:00Z</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-08-29T00:00:00Z</td>\n",
       "      <td>...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>http://www.amazon.com/Amazon-PowerFast-Adapter...</td>\n",
       "      <td>This is not appreciably faster than any other ...</td>\n",
       "      <td>Not appreciably faster than any other 1.8A cha...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Kris</td>\n",
       "      <td>1</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34656</th>\n",
       "      <td>AVpfiBlyLJeJML43-4Tp</td>\n",
       "      <td>NaN</td>\n",
       "      <td>B006GWO5WK</td>\n",
       "      <td>Amazon</td>\n",
       "      <td>Computers/Tablets &amp; Networking,Tablet &amp; eBook ...</td>\n",
       "      <td>newamazonkindlefirehd9wpowerfastadaptercharger...</td>\n",
       "      <td>Amazon Digital Services, Inc</td>\n",
       "      <td>2012-11-21T00:00:00Z</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-09-02T00:00:00Z</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>http://www.amazon.com/Amazon-PowerFast-Adapter...</td>\n",
       "      <td>Amazon should include this charger with the Ki...</td>\n",
       "      <td>Should be included</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Jonathan Stewart</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34657</th>\n",
       "      <td>AVpfiBlyLJeJML43-4Tp</td>\n",
       "      <td>NaN</td>\n",
       "      <td>B006GWO5WK</td>\n",
       "      <td>Amazon</td>\n",
       "      <td>Computers/Tablets &amp; Networking,Tablet &amp; eBook ...</td>\n",
       "      <td>newamazonkindlefirehd9wpowerfastadaptercharger...</td>\n",
       "      <td>Amazon Digital Services, Inc</td>\n",
       "      <td>2012-10-19T00:00:00Z</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-09-04T00:00:00Z</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>http://www.amazon.com/Amazon-PowerFast-Adapter...</td>\n",
       "      <td>Love my Kindle Fire but I am really disappoint...</td>\n",
       "      <td>Disappointing Charger</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>J Lawson</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34658</th>\n",
       "      <td>AVpfiBlyLJeJML43-4Tp</td>\n",
       "      <td>NaN</td>\n",
       "      <td>B006GWO5WK</td>\n",
       "      <td>Amazon</td>\n",
       "      <td>Computers/Tablets &amp; Networking,Tablet &amp; eBook ...</td>\n",
       "      <td>newamazonkindlefirehd9wpowerfastadaptercharger...</td>\n",
       "      <td>Amazon Digital Services, Inc</td>\n",
       "      <td>2012-10-31T00:00:00Z</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-09-01T00:00:00Z</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>http://www.amazon.com/Amazon-PowerFast-Adapter...</td>\n",
       "      <td>I was surprised to find it did not come with a...</td>\n",
       "      <td>Not worth the money</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just the Buyer</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34659</th>\n",
       "      <td>AVpfiBlyLJeJML43-4Tp</td>\n",
       "      <td>NaN</td>\n",
       "      <td>B006GWO5WK</td>\n",
       "      <td>Amazon</td>\n",
       "      <td>Computers/Tablets &amp; Networking,Tablet &amp; eBook ...</td>\n",
       "      <td>newamazonkindlefirehd9wpowerfastadaptercharger...</td>\n",
       "      <td>Amazon Digital Services, Inc</td>\n",
       "      <td>2012-12-23T00:00:00Z</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-11-01T00:00:00Z</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>http://www.amazon.com/Amazon-PowerFast-Adapter...</td>\n",
       "      <td>to spite the fact that i have nothing but good...</td>\n",
       "      <td>as with everyone else</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>sandi</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>34659 rows × 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                         id  \\\n",
       "0      AVqkIhwDv8e3D1O-lebb   \n",
       "1      AVqkIhwDv8e3D1O-lebb   \n",
       "2      AVqkIhwDv8e3D1O-lebb   \n",
       "3      AVqkIhwDv8e3D1O-lebb   \n",
       "4      AVqkIhwDv8e3D1O-lebb   \n",
       "...                     ...   \n",
       "34655  AVpfiBlyLJeJML43-4Tp   \n",
       "34656  AVpfiBlyLJeJML43-4Tp   \n",
       "34657  AVpfiBlyLJeJML43-4Tp   \n",
       "34658  AVpfiBlyLJeJML43-4Tp   \n",
       "34659  AVpfiBlyLJeJML43-4Tp   \n",
       "\n",
       "                                                    name       asins   brand  \\\n",
       "0      All-New Fire HD 8 Tablet, 8 HD Display, Wi-Fi,...  B01AHB9CN2  Amazon   \n",
       "1      All-New Fire HD 8 Tablet, 8 HD Display, Wi-Fi,...  B01AHB9CN2  Amazon   \n",
       "2      All-New Fire HD 8 Tablet, 8 HD Display, Wi-Fi,...  B01AHB9CN2  Amazon   \n",
       "3      All-New Fire HD 8 Tablet, 8 HD Display, Wi-Fi,...  B01AHB9CN2  Amazon   \n",
       "4      All-New Fire HD 8 Tablet, 8 HD Display, Wi-Fi,...  B01AHB9CN2  Amazon   \n",
       "...                                                  ...         ...     ...   \n",
       "34655                                                NaN  B006GWO5WK  Amazon   \n",
       "34656                                                NaN  B006GWO5WK  Amazon   \n",
       "34657                                                NaN  B006GWO5WK  Amazon   \n",
       "34658                                                NaN  B006GWO5WK  Amazon   \n",
       "34659                                                NaN  B006GWO5WK  Amazon   \n",
       "\n",
       "                                              categories  \\\n",
       "0      Electronics,iPad & Tablets,All Tablets,Fire Ta...   \n",
       "1      Electronics,iPad & Tablets,All Tablets,Fire Ta...   \n",
       "2      Electronics,iPad & Tablets,All Tablets,Fire Ta...   \n",
       "3      Electronics,iPad & Tablets,All Tablets,Fire Ta...   \n",
       "4      Electronics,iPad & Tablets,All Tablets,Fire Ta...   \n",
       "...                                                  ...   \n",
       "34655  Computers/Tablets & Networking,Tablet & eBook ...   \n",
       "34656  Computers/Tablets & Networking,Tablet & eBook ...   \n",
       "34657  Computers/Tablets & Networking,Tablet & eBook ...   \n",
       "34658  Computers/Tablets & Networking,Tablet & eBook ...   \n",
       "34659  Computers/Tablets & Networking,Tablet & eBook ...   \n",
       "\n",
       "                                                    keys  \\\n",
       "0      841667104676,amazon/53004484,amazon/b01ahb9cn2...   \n",
       "1      841667104676,amazon/53004484,amazon/b01ahb9cn2...   \n",
       "2      841667104676,amazon/53004484,amazon/b01ahb9cn2...   \n",
       "3      841667104676,amazon/53004484,amazon/b01ahb9cn2...   \n",
       "4      841667104676,amazon/53004484,amazon/b01ahb9cn2...   \n",
       "...                                                  ...   \n",
       "34655  newamazonkindlefirehd9wpowerfastadaptercharger...   \n",
       "34656  newamazonkindlefirehd9wpowerfastadaptercharger...   \n",
       "34657  newamazonkindlefirehd9wpowerfastadaptercharger...   \n",
       "34658  newamazonkindlefirehd9wpowerfastadaptercharger...   \n",
       "34659  newamazonkindlefirehd9wpowerfastadaptercharger...   \n",
       "\n",
       "                       manufacturer              reviews.date  \\\n",
       "0                            Amazon  2017-01-13T00:00:00.000Z   \n",
       "1                            Amazon  2017-01-13T00:00:00.000Z   \n",
       "2                            Amazon  2017-01-13T00:00:00.000Z   \n",
       "3                            Amazon  2017-01-13T00:00:00.000Z   \n",
       "4                            Amazon  2017-01-12T00:00:00.000Z   \n",
       "...                             ...                       ...   \n",
       "34655  Amazon Digital Services, Inc      2012-09-18T00:00:00Z   \n",
       "34656  Amazon Digital Services, Inc      2012-11-21T00:00:00Z   \n",
       "34657  Amazon Digital Services, Inc      2012-10-19T00:00:00Z   \n",
       "34658  Amazon Digital Services, Inc      2012-10-31T00:00:00Z   \n",
       "34659  Amazon Digital Services, Inc      2012-12-23T00:00:00Z   \n",
       "\n",
       "          reviews.dateAdded  \\\n",
       "0      2017-07-03T23:33:15Z   \n",
       "1      2017-07-03T23:33:15Z   \n",
       "2      2017-07-03T23:33:15Z   \n",
       "3      2017-07-03T23:33:15Z   \n",
       "4      2017-07-03T23:33:15Z   \n",
       "...                     ...   \n",
       "34655                   NaN   \n",
       "34656                   NaN   \n",
       "34657                   NaN   \n",
       "34658                   NaN   \n",
       "34659                   NaN   \n",
       "\n",
       "                                        reviews.dateSeen  ... reviews.rating  \\\n",
       "0      2017-06-07T09:04:00.000Z,2017-04-30T00:45:00.000Z  ...            5.0   \n",
       "1      2017-06-07T09:04:00.000Z,2017-04-30T00:45:00.000Z  ...            5.0   \n",
       "2      2017-06-07T09:04:00.000Z,2017-04-30T00:45:00.000Z  ...            5.0   \n",
       "3      2017-06-07T09:04:00.000Z,2017-04-30T00:45:00.000Z  ...            4.0   \n",
       "4      2017-06-07T09:04:00.000Z,2017-04-30T00:45:00.000Z  ...            5.0   \n",
       "...                                                  ...  ...            ...   \n",
       "34655                               2015-08-29T00:00:00Z  ...            3.0   \n",
       "34656                               2015-09-02T00:00:00Z  ...            1.0   \n",
       "34657                               2015-09-04T00:00:00Z  ...            1.0   \n",
       "34658                               2015-09-01T00:00:00Z  ...            1.0   \n",
       "34659                               2015-11-01T00:00:00Z  ...            1.0   \n",
       "\n",
       "                                      reviews.sourceURLs  \\\n",
       "0      http://reviews.bestbuy.com/3545/5620406/review...   \n",
       "1      http://reviews.bestbuy.com/3545/5620406/review...   \n",
       "2      http://reviews.bestbuy.com/3545/5620406/review...   \n",
       "3      http://reviews.bestbuy.com/3545/5620406/review...   \n",
       "4      http://reviews.bestbuy.com/3545/5620406/review...   \n",
       "...                                                  ...   \n",
       "34655  http://www.amazon.com/Amazon-PowerFast-Adapter...   \n",
       "34656  http://www.amazon.com/Amazon-PowerFast-Adapter...   \n",
       "34657  http://www.amazon.com/Amazon-PowerFast-Adapter...   \n",
       "34658  http://www.amazon.com/Amazon-PowerFast-Adapter...   \n",
       "34659  http://www.amazon.com/Amazon-PowerFast-Adapter...   \n",
       "\n",
       "                                            reviews.text  \\\n",
       "0      This product so far has not disappointed. My c...   \n",
       "1      great for beginner or experienced person. Boug...   \n",
       "2      Inexpensive tablet for him to use and learn on...   \n",
       "3      I've had my Fire HD 8 two weeks now and I love...   \n",
       "4      I bought this for my grand daughter when she c...   \n",
       "...                                                  ...   \n",
       "34655  This is not appreciably faster than any other ...   \n",
       "34656  Amazon should include this charger with the Ki...   \n",
       "34657  Love my Kindle Fire but I am really disappoint...   \n",
       "34658  I was surprised to find it did not come with a...   \n",
       "34659  to spite the fact that i have nothing but good...   \n",
       "\n",
       "                                           reviews.title  reviews.userCity  \\\n",
       "0                                                 Kindle               NaN   \n",
       "1                                              very fast               NaN   \n",
       "2                Beginner tablet for our 9 year old son.               NaN   \n",
       "3                                                Good!!!               NaN   \n",
       "4                              Fantastic Tablet for kids               NaN   \n",
       "...                                                  ...               ...   \n",
       "34655  Not appreciably faster than any other 1.8A cha...               NaN   \n",
       "34656                                 Should be included               NaN   \n",
       "34657                              Disappointing Charger               NaN   \n",
       "34658                                Not worth the money               NaN   \n",
       "34659                              as with everyone else               NaN   \n",
       "\n",
       "      reviews.userProvince  reviews.username labels  labels_old  sentiment  \n",
       "0                      NaN           Adapter      2         5.0          2  \n",
       "1                      NaN            truman      2         5.0          2  \n",
       "2                      NaN             DaveZ      2         5.0          2  \n",
       "3                      NaN            Shacks      2         4.0          2  \n",
       "4                      NaN         explore42      2         5.0          2  \n",
       "...                    ...               ...    ...         ...        ...  \n",
       "34655                  NaN              Kris      1         3.0          1  \n",
       "34656                  NaN  Jonathan Stewart      0         1.0          0  \n",
       "34657                  NaN          J Lawson      0         1.0          0  \n",
       "34658                  NaN    Just the Buyer      0         1.0          0  \n",
       "34659                  NaN             sandi      0         1.0          0  \n",
       "\n",
       "[34659 rows x 24 columns]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
